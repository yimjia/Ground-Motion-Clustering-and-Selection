{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder for Non-Pulse-Type GMs\n",
    "# <font color='blue'>Pre-trained<font color='black'> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File directory (change the directory to the folder including GMIM_NonPulse_Pretrain.mat)\n",
    "file_dir = 'C:/Users/jiayi/Desktop/GM Clustering and Selection Codes for GitHub/'\n",
    "\n",
    "\n",
    "Pulse_Type = \"NonPulse\"\n",
    "\n",
    "\n",
    "Model_Type = \"Pretrain\"\n",
    "\n",
    "\n",
    "# Stop training when loss reaching\n",
    "Loss_Stop = 0.005\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCH = 10000\n",
    "\n",
    "\n",
    "# Batch size (default as 5 times of the number of non-pulse-type GMs, user can select other batch size based on experience)\n",
    "BATCH_SIZE = 180\n",
    "\n",
    "\n",
    "# Learning rate for Adam optimizer\n",
    "Learning_Rate = 0.001\n",
    "\n",
    "\n",
    "# Drop learning rate\n",
    "Drop_Rate1 = 0.85\n",
    "NUM_EPOCH_DROP1 = 1000\n",
    "\n",
    "Drop_Rate2 = 0.90\n",
    "NUM_EPOCH_DROP2 = 2000\n",
    "\n",
    "Drop_Rate3 = 0.95\n",
    "NUM_EPOCH_DROP3 = 3000\n",
    "\n",
    "Drop_Rate4 = 1.00\n",
    "\n",
    "\n",
    "# Drop learning rate every Drop_Epoch\n",
    "Drop_Epoch = 250\n",
    "\n",
    "\n",
    "# Latent space size\n",
    "latent_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import PReLU\n",
    "from keras.layers import LeakyReLU\n",
    "# Activation function other than latent space \n",
    "# act_fun = 'ReLU'\n",
    "# act_fun = 'PReLU'\n",
    "act_fun = LeakyReLU(alpha=0.3)\n",
    "# act_fun = 'tanh'\n",
    "\n",
    "# Activation function for latent space \n",
    "# act_fun_LS = 'ReLU'\n",
    "# act_fun_LS = 'PReLU'\n",
    "act_fun_LS = LeakyReLU(alpha=0.3)\n",
    "# act_fun_LS = 'tanh'\n",
    "\n",
    "\n",
    "# Kernel size\n",
    "KERNEL_SIZE1 = 10\n",
    "KERNEL_SIZE2 = 5\n",
    "\n",
    "\n",
    "# Max pooling size\n",
    "POOL_SIZE1 = 5\n",
    "POOL_SIZE2 = 3\n",
    "\n",
    "\n",
    "# Load Sa data\n",
    "import scipy.io\n",
    "Data_Name = 'GMIM_' + Pulse_Type + '_' + Model_Type + '.mat' \n",
    "# print('Input GMs = ' + Data_Name)\n",
    "GMIM_Raw = scipy.io.loadmat(file_dir + Data_Name)\n",
    "\n",
    "\n",
    "# Define a GPU usage (default as CPU)\n",
    "# CPU = -1, GPU0 = 0\n",
    "GPU_ID = -1\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID) \n",
    "\n",
    "\n",
    "# Folder name\n",
    "ID = f\"_{Pulse_Type}_{Model_Type}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the GPU is specified\n",
    "import tensorflow as tf\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for model, data, and figure\n",
    "import shutil\n",
    "\n",
    "model_dir = f\"Model{str(ID)}\" \n",
    "modelExist = os.path.exists(model_dir)\n",
    "if modelExist:\n",
    "    shutil.rmtree(model_dir)\n",
    "    os.makedirs(model_dir) \n",
    "    # print('Model folder exists, delete the current files in this folder.')\n",
    "else:\n",
    "    os.makedirs(model_dir)    \n",
    "    # print('Model folder does not exist, create this folder.')\n",
    "    \n",
    "data_dir = f\"Data{str(ID)}\" \n",
    "dataExist = os.path.exists(data_dir)\n",
    "if dataExist:\n",
    "    shutil.rmtree(data_dir)\n",
    "    os.makedirs(data_dir) \n",
    "    # print('Data folder exists, delete the current files in this folder.')\n",
    "else:\n",
    "    os.makedirs(data_dir)    \n",
    "    # print('Data folder does not exist, create this folder.')\n",
    "        \n",
    "figure_dir = f\"Figure{str(ID)}\" \n",
    "figureExist = os.path.exists(figure_dir)\n",
    "if figureExist:\n",
    "    shutil.rmtree(figure_dir)    \n",
    "    os.makedirs(figure_dir) \n",
    "    # print('Figure folder exists, delete the current files in this folder.')\n",
    "else:\n",
    "    os.makedirs(figure_dir)    \n",
    "    # print('Figure folder does not exist, create this folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten, Reshape, Conv1DTranspose, Lambda, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sa\n",
    "Sa = GMIM_Raw[\"LogSa\"]     \n",
    "\n",
    "# Check the dimensions of Sa\n",
    "print('Sa   shape:', Sa.shape)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Sa\n",
    "input_Sa = Input(shape=(Sa.shape[1],1), name = 'x_Sa')\n",
    "\n",
    "\n",
    "# Convolutional and max pooling layers for Sa\n",
    "x_Sa_C1 = Conv1D(filters = 32, kernel_size = KERNEL_SIZE1, padding=\"same\",\n",
    "                 activation=act_fun, name = 'x_Sa_C1',\n",
    "                 kernel_initializer=initializers.HeNormal,\n",
    "                 bias_initializer=initializers.Zeros())(input_Sa)\n",
    "\n",
    "x_Sa_P1 = MaxPooling1D(pool_size=POOL_SIZE1, strides=2, padding='same', name = 'x_Sa_P1')(x_Sa_C1)\n",
    "\n",
    "x_Sa_C2 = Conv1D(filters = 16, kernel_size = KERNEL_SIZE1, padding=\"same\", \n",
    "                 activation=act_fun, name = 'x_Sa_C2',\n",
    "                 kernel_initializer=initializers.HeNormal,\n",
    "                 bias_initializer=initializers.Zeros())(x_Sa_P1)\n",
    "\n",
    "x_Sa_P2 = MaxPooling1D(pool_size=POOL_SIZE2, strides=2, padding='same', name = 'x_Sa_P2')(x_Sa_C2)\n",
    "\n",
    "x_Sa_C3 = Conv1D(filters = 8, kernel_size = KERNEL_SIZE2, padding=\"same\",\n",
    "                 activation=act_fun, name = 'x_Sa_C3',\n",
    "                 kernel_initializer=initializers.HeNormal,\n",
    "                 bias_initializer=initializers.Zeros())(x_Sa_P2)\n",
    "\n",
    "# Flatten layer for Sa\n",
    "x_Sa_F = Flatten(name = 'x_Sa_F')(x_Sa_C3)\n",
    "\n",
    "\n",
    "# Dense layers for Sa\n",
    "x_Sa_D1 = Dense(256, activation=act_fun, name = 'x_Sa_D1',\n",
    "                      kernel_initializer=initializers.HeNormal,\n",
    "                      bias_initializer=initializers.Zeros())(x_Sa_F)\n",
    "x_Sa_D2 = Dense(128, activation=act_fun, name = 'x_Sa_D2',\n",
    "                     kernel_initializer=initializers.HeNormal,\n",
    "                     bias_initializer=initializers.Zeros())(x_Sa_D1)\n",
    "x_Sa_D3 = Dense(64, activation=act_fun, name = 'x_Sa_D3',\n",
    "                    kernel_initializer=initializers.HeNormal,\n",
    "                    bias_initializer=initializers.Zeros())(x_Sa_D2)\n",
    "x_Sa_D4 = Dense(32, activation=act_fun, name = 'x_Sa_D4',\n",
    "                    kernel_initializer=initializers.HeNormal,\n",
    "                    bias_initializer=initializers.Zeros())(x_Sa_D3)\n",
    "x_Sa_D5 = Dense(16, activation=act_fun, name = 'x_Sa_D5',\n",
    "                    kernel_initializer=initializers.HeNormal,\n",
    "                    bias_initializer=initializers.Zeros())(x_Sa_D4)\n",
    "x_Sa_D6 = Dense(8, activation=act_fun, name = 'x_Sa_D6',\n",
    "                   kernel_initializer=initializers.HeNormal,\n",
    "                   bias_initializer=initializers.Zeros())(x_Sa_D5)\n",
    "\n",
    "\n",
    "# Latent feature layer\n",
    "z = Dense(latent_dim, activation=act_fun_LS, name = 'Encoder',\n",
    "                      kernel_initializer=initializers.HeNormal,\n",
    "                      bias_initializer=initializers.Zeros())(x_Sa_D6)\n",
    "\n",
    "\n",
    "# Denses layer for Sa\n",
    "y_Sa_D1 = Dense(8, activation=act_fun, name = 'y_Sa_D1',\n",
    "                   kernel_initializer=initializers.HeNormal,\n",
    "                   bias_initializer=initializers.Zeros())(z)\n",
    "y_Sa_D2 = Dense(16, activation=act_fun, name = 'y_Sa_D2',\n",
    "                    kernel_initializer=initializers.HeNormal,\n",
    "                    bias_initializer=initializers.Zeros())(y_Sa_D1)\n",
    "y_Sa_D3 = Dense(32, activation=act_fun, name = 'y_Sa_D3',\n",
    "                    kernel_initializer=initializers.HeNormal,\n",
    "                    bias_initializer=initializers.Zeros())(y_Sa_D2)\n",
    "y_Sa_D4 = Dense(64, activation=act_fun, name = 'y_Sa_D4',\n",
    "                    kernel_initializer=initializers.HeNormal,\n",
    "                    bias_initializer=initializers.Zeros())(y_Sa_D3)\n",
    "y_Sa_D5 = Dense(128, activation=act_fun, name = 'y_Sa_D5',\n",
    "                     kernel_initializer=initializers.HeNormal,\n",
    "                     bias_initializer=initializers.Zeros())(y_Sa_D4)\n",
    "y_Sa_D6 = Dense(256, activation=act_fun, name = 'y_Sa_D6',\n",
    "                     kernel_initializer=initializers.HeNormal,\n",
    "                     bias_initializer=initializers.Zeros())(y_Sa_D5)\n",
    "y_Sa_D7 = Dense(x_Sa_F.shape[1], activation=act_fun, name = 'y_Sa_D7',\n",
    "                                 kernel_initializer=initializers.HeNormal,\n",
    "                                 bias_initializer=initializers.Zeros())(y_Sa_D6)\n",
    "\n",
    "\n",
    "# Inverse flatten layer\n",
    "y_Sa_F = Reshape([round(Sa.shape[1]/4), 8], name = 'y_Sa_F')(y_Sa_D7)\n",
    "\n",
    "\n",
    "# Deconvolutional layers\n",
    "y_Sa_C1 = Conv1DTranspose(filters = 16, kernel_size = KERNEL_SIZE2, padding=\"same\", \n",
    "                          activation=act_fun, name = 'y_Sa_C1',\n",
    "                          kernel_initializer=initializers.HeNormal,\n",
    "                          bias_initializer=initializers.Zeros())(y_Sa_F)\n",
    "\n",
    "y_Sa_C2 = Conv1DTranspose(filters = 16, strides = 2, kernel_size = KERNEL_SIZE2, padding=\"same\", \n",
    "                          activation=act_fun, name = 'y_Sa_C2',\n",
    "                          kernel_initializer=initializers.HeNormal,\n",
    "                          bias_initializer=initializers.Zeros())(y_Sa_C1)                                                     \n",
    "\n",
    "y_Sa_C3 = Conv1DTranspose(filters = 32, kernel_size = KERNEL_SIZE1, padding=\"same\", \n",
    "                          activation=act_fun, name = 'y_Sa_C3',\n",
    "                          kernel_initializer=initializers.HeNormal,\n",
    "                          bias_initializer=initializers.Zeros())(y_Sa_C2)                                                     \n",
    "\n",
    "y_Sa_C4 = Conv1DTranspose(filters = 32, strides = 2, kernel_size = KERNEL_SIZE1, padding=\"same\", \n",
    "                          activation=act_fun, name = 'y_Sa_C4',\n",
    "                          kernel_initializer=initializers.HeNormal,\n",
    "                          bias_initializer=initializers.Zeros())(y_Sa_C3)   \n",
    "\n",
    "y_Sa_C5 = Conv1DTranspose(filters = 1, kernel_size = KERNEL_SIZE1, padding=\"same\", \n",
    "                          activation=act_fun, name = 'y_Sa_C5',\n",
    "                          kernel_initializer=initializers.HeNormal,\n",
    "                          bias_initializer=initializers.Zeros())(y_Sa_C4)    \n",
    "\n",
    "output_Sa = Conv1DTranspose(filters = 1, kernel_size = KERNEL_SIZE1, padding=\"same\", \n",
    "                            activation='linear', name = 'y_Sa',\n",
    "                            kernel_initializer=initializers.HeNormal,\n",
    "                            bias_initializer=initializers.Zeros())(y_Sa_C5) \n",
    "\n",
    "\n",
    "autoencoder = Model([input_Sa], [output_Sa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(autoencoder, figure_dir + \"/Autoencoder.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "loss1 = mse(input_Sa, output_Sa)\n",
    "autoencoder.add_loss(loss1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom optimizer leanring rate\n",
    "autoencoder.compile(optimizer=optimizers.legacy.Adam(learning_rate=Learning_Rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leanring rate step decay\n",
    "def lr_step_decay(epoch, lr):\n",
    "    if epoch < NUM_EPOCH_DROP1:\n",
    "        return Learning_Rate * math.pow(Drop_Rate1, math.floor(epoch/Drop_Epoch))\n",
    "    if ((epoch >= NUM_EPOCH_DROP1) and (epoch < NUM_EPOCH_DROP2)):\n",
    "        return Learning_Rate * math.pow(Drop_Rate1, math.floor(NUM_EPOCH_DROP1/Drop_Epoch)) * math.pow(Drop_Rate2, math.floor((epoch-NUM_EPOCH_DROP1)/Drop_Epoch))\n",
    "    if ((epoch >= NUM_EPOCH_DROP2) and (epoch < NUM_EPOCH_DROP3)):\n",
    "        return Learning_Rate * math.pow(Drop_Rate1, math.floor(NUM_EPOCH_DROP1/Drop_Epoch)) * math.pow(Drop_Rate2, math.floor((NUM_EPOCH_DROP2-NUM_EPOCH_DROP1)/Drop_Epoch)) * math.pow(Drop_Rate3, math.floor((epoch-NUM_EPOCH_DROP2)/Drop_Epoch))\n",
    "    if epoch >= NUM_EPOCH_DROP3:\n",
    "        return Learning_Rate * math.pow(Drop_Rate1, math.floor(NUM_EPOCH_DROP1/Drop_Epoch)) * math.pow(Drop_Rate2, math.floor((NUM_EPOCH_DROP2-NUM_EPOCH_DROP1)/Drop_Epoch)) * math.pow(Drop_Rate3, math.floor((NUM_EPOCH_DROP3-NUM_EPOCH_DROP2)/Drop_Epoch)) * math.pow(Drop_Rate4, math.floor((epoch-NUM_EPOCH_DROP3)/Drop_Epoch))\n",
    "\n",
    "              \n",
    "LR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint path and directory\n",
    "checkpoint_path = model_dir + \"/Checkpoint.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to save the model's weights at the best epoch\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path,\n",
    "                                                 verbose = 1, \n",
    "                                                 monitor='loss',\n",
    "                                                 mode='min',\n",
    "                                                 save_weights_only = True,\n",
    "                                                 save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training when loss reaches the user-defined value\n",
    "class haltCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('loss') <= Loss_Stop):\n",
    "            print(\"\\n\\n\\nReached %s loss value, so stopping training!\\n\\n\\n\"  %(str(Loss_Stop)))\n",
    "            self.model.stop_training = True\n",
    "\n",
    "trainingStopCallback = haltCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "Input_set = [Sa.reshape(Sa.shape[0], Sa.shape[1],1)]\n",
    "Output_set = [Sa.reshape(Sa.shape[0], Sa.shape[1],1)]\n",
    "\n",
    "start = time.time()\n",
    "history = autoencoder.fit(Input_set, Output_set,\n",
    "                          epochs = NUM_EPOCH,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          callbacks = [cp_callback, trainingStopCallback, LR_Scheduler],\n",
    "                          verbose = 0)\n",
    "\n",
    "        \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running time\n",
    "print(\"The running time:\", round((end-start)/60,1), \"mins\")\n",
    "print('')\n",
    "\n",
    "Time = np.asarray(round((end-start)/60,1))\n",
    "Time = Time.reshape(1,-1)\n",
    "np.savetxt(data_dir + \"/Running_Time.txt\", Time);\n",
    "\n",
    "# Find the best number of epochs to avoid overfitting\n",
    "val_loss_min_index = history.history['loss']. index(min(history.history['loss']))\n",
    "ckpt_best = val_loss_min_index+1\n",
    "print('Number of epochs at the minimum loss = ', ckpt_best)\n",
    "print('')\n",
    "\n",
    "Min_Test_Loss_Epoch = open(data_dir + '/Min_Loss_Epoch.dat', 'w')\n",
    "Min_Test_Loss_Epoch.write(str(ckpt_best))\n",
    "Min_Test_Loss_Epoch.close()\n",
    "\n",
    "# Save loss\n",
    "np.savetxt(data_dir + '/Loss.dat', history.history['loss'])\n",
    "\n",
    "# Plot loss\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(np.arange(1,len(history.history['loss'])+1), history.history['loss'],'k-',linewidth=2,label='Train loss')\n",
    "plt.legend(fontsize=20)\n",
    "\n",
    "plt.xlim([0, len(history.history['loss'])+1])\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=20)\n",
    "ax.set_ylabel('Loss', fontsize=20) \n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "\n",
    "fig.set_size_inches(10, 7)\n",
    "\n",
    "plt.savefig(figure_dir + '/Loss.jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best checkpoint ID\n",
    "Checkpoint_ID = os.path.join(model_dir, \"Checkpoint.ckpt\")\n",
    "\n",
    "# Save decoded data\n",
    "autoencoder_ckpt_best = autoencoder;\n",
    "autoencoder_ckpt_best.load_weights(Checkpoint_ID);\n",
    "All_decoded = autoencoder_ckpt_best.predict([Sa.reshape(Sa.shape[0], Sa.shape[1],1)],verbose = 0);\n",
    "Sa_decoded = All_decoded.reshape(All_decoded.shape[0], All_decoded.shape[1]);\n",
    "np.savetxt(data_dir + '/Decode_Sa.dat', Sa_decoded);\n",
    "\n",
    "# Save encoded data\n",
    "layer_name_Encoder = 'Encoder'\n",
    "encoder_ckpt_best_Encoder = Model(inputs = autoencoder_ckpt_best.input, outputs = autoencoder_ckpt_best.get_layer(layer_name_Encoder).output);\n",
    "All_encoded_Encoder = encoder_ckpt_best_Encoder.predict([Sa.reshape(Sa.shape[0], Sa.shape[1],1)],verbose = 0);\n",
    "np.savetxt(data_dir + '/Encode.dat', All_encoded_Encoder);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sa\n",
    "fig, ax = plt.subplots()\n",
    "In_plot = np.exp(np.array(Sa).flatten());\n",
    "Out_plot = np.exp(np.array(Sa_decoded).flatten());\n",
    "Rho = np.corrcoef(In_plot,Out_plot) \n",
    "MSE = np.square(np.subtract(In_plot,Out_plot)).mean()\n",
    "\n",
    "plt.xlim([min(min(In_plot),min(Out_plot))-0.05, max(max(In_plot),max(Out_plot))+0.05])\n",
    "plt.ylim([min(min(In_plot),min(Out_plot))-0.05, max(max(In_plot),max(Out_plot))+0.05])\n",
    "ax.plot([min(min(In_plot),min(Out_plot))-0.05, max(max(In_plot),max(Out_plot))+0.05], [min(min(In_plot),min(Out_plot))-0.05, max(max(In_plot),max(Out_plot))+0.05], \n",
    "        ls=\"--\", color = \".3\")\n",
    "\n",
    "ax.plot([0, 0], [min(min(In_plot),min(Out_plot))-0.05, max(max(In_plot),max(Out_plot))+0.05], ls=\"-.\", color =\".3\")\n",
    "ax.plot([min(min(In_plot),min(Out_plot))-0.05, max(max(In_plot),max(Out_plot))+0.05], [0, 0], ls=\"-.\", color =\".3\")\n",
    "\n",
    "\n",
    "plt.scatter(In_plot, Out_plot, marker='o', color=\"b\", alpha=0.3, label= r'$\\rho$ = %.4f,  MSE = %.4f' %(Rho[0,1], MSE))\n",
    "plt.legend(fontsize=20,loc='upper left')\n",
    "\n",
    "ax.set_xlabel('Input', fontsize=20)\n",
    "ax.set_ylabel('Output', fontsize=20) \n",
    "\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.set_title('Sa', fontsize=20)\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "plt.savefig(figure_dir + '/Q-Q Sa.jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
